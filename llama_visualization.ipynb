{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "1G2A0AJEyIswE5uLHOcHuMOaZWqYQdf4f",
      "authorship_tag": "ABX9TyMUGT9gP15Xmn3sjs6LuMVG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7c2a4d51a975451e937d475353eb5a37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ba3d655a05a249edaf2d942585623e51",
              "IPY_MODEL_64d8261ef91d4b709e87cd932d7c1fd2",
              "IPY_MODEL_72677ad9909a4c09ac0d229becdfeb0c"
            ],
            "layout": "IPY_MODEL_7d7cd0fd8cf540a681cb8698d96d13ae"
          }
        },
        "ba3d655a05a249edaf2d942585623e51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4eddf9a5bce940a1b287cd5318c707a8",
            "placeholder": "​",
            "style": "IPY_MODEL_895fc91f1e904e158de4cfe998d0adde",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "64d8261ef91d4b709e87cd932d7c1fd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5a7215396ac4da49bb3eda017386343",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4f8ac662fdbc40129ac25d9a40e5fdb6",
            "value": 2
          }
        },
        "72677ad9909a4c09ac0d229becdfeb0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7d8d3a0da0345cea644cfd43fd2bca2",
            "placeholder": "​",
            "style": "IPY_MODEL_923a0bdcc80f49bcb777865cf6e2af6c",
            "value": " 2/2 [00:05&lt;00:00,  2.49s/it]"
          }
        },
        "7d7cd0fd8cf540a681cb8698d96d13ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4eddf9a5bce940a1b287cd5318c707a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "895fc91f1e904e158de4cfe998d0adde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e5a7215396ac4da49bb3eda017386343": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f8ac662fdbc40129ac25d9a40e5fdb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c7d8d3a0da0345cea644cfd43fd2bca2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "923a0bdcc80f49bcb777865cf6e2af6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "84a3369286a544ff872ab33d8d2a4283": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_66d5fbb11d3044d68f09cd1840092c41",
              "IPY_MODEL_8e5a2039827e40b2962b377aa04f6c24",
              "IPY_MODEL_0d55ccfd8d8e44dab60c480f64c12308"
            ],
            "layout": "IPY_MODEL_8da8a3523c8a41ab95ff5631a92e1b68"
          }
        },
        "66d5fbb11d3044d68f09cd1840092c41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e62df388f6d1419a9887e34a94b9f2aa",
            "placeholder": "​",
            "style": "IPY_MODEL_94bf0e0be85647cc8ff3efecaab2d651",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "8e5a2039827e40b2962b377aa04f6c24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_534c34aaf015439ba8dcf12361a2bd7d",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_58f8cea9f9dd474aa7b9932a156a9ffc",
            "value": 2
          }
        },
        "0d55ccfd8d8e44dab60c480f64c12308": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aead77c155074c2999484ca4cb2e2af6",
            "placeholder": "​",
            "style": "IPY_MODEL_5a010f34606a49bb84494eeab420b736",
            "value": " 2/2 [00:04&lt;00:00,  2.15s/it]"
          }
        },
        "8da8a3523c8a41ab95ff5631a92e1b68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e62df388f6d1419a9887e34a94b9f2aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94bf0e0be85647cc8ff3efecaab2d651": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "534c34aaf015439ba8dcf12361a2bd7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58f8cea9f9dd474aa7b9932a156a9ffc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aead77c155074c2999484ca4cb2e2af6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a010f34606a49bb84494eeab420b736": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laerdon/llama-model-analysis/blob/main/llama_visualization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMVI0tYv8ziY",
        "outputId": "7f50d265-098a-49cf-a088-1a483cae99c2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install --upgrade transformers\n",
        "! pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqFz4Z9W9EcJ",
        "outputId": "6dd733cb-eb72-4013-bfd2-bbc9c7f9b108"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.44.0-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Downloading transformers-4.44.0-py3-none-any.whl (9.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m121.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.42.4\n",
            "    Uninstalling transformers-4.42.4:\n",
            "      Successfully uninstalled transformers-4.42.4\n",
            "Successfully installed transformers-4.44.0\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.3.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-2.21.0-py3-none-any.whl (527 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, pyarrow, dill, multiprocess, datasets\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.21.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-17.0.0 xxhash-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -U bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAm-64KaWu6n",
        "outputId": "20e45639-f29e-49eb-f7e9-10ef55d8069b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.43.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.3.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes) (12.6.20)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # prompt: mount drive\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ltV9JNZwdboc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OYtmq_H4xfQZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "7c2a4d51a975451e937d475353eb5a37",
            "ba3d655a05a249edaf2d942585623e51",
            "64d8261ef91d4b709e87cd932d7c1fd2",
            "72677ad9909a4c09ac0d229becdfeb0c",
            "7d7cd0fd8cf540a681cb8698d96d13ae",
            "4eddf9a5bce940a1b287cd5318c707a8",
            "895fc91f1e904e158de4cfe998d0adde",
            "e5a7215396ac4da49bb3eda017386343",
            "4f8ac662fdbc40129ac25d9a40e5fdb6",
            "c7d8d3a0da0345cea644cfd43fd2bca2",
            "923a0bdcc80f49bcb777865cf6e2af6c"
          ]
        },
        "outputId": "7493ccd0-4c17-4f04-fe63-203be48be7cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7c2a4d51a975451e937d475353eb5a37"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# # install LLaMA 2\n",
        "# # Load model directly\n",
        "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install CUSTOM LLaMA 2\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from drive.MyDrive.llama_analysis.modeling_llama import LlamaForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", trust_remote_token=True, token=\"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\")\n",
        "model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", load_in_8bit=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208,
          "referenced_widgets": [
            "84a3369286a544ff872ab33d8d2a4283",
            "66d5fbb11d3044d68f09cd1840092c41",
            "8e5a2039827e40b2962b377aa04f6c24",
            "0d55ccfd8d8e44dab60c480f64c12308",
            "8da8a3523c8a41ab95ff5631a92e1b68",
            "e62df388f6d1419a9887e34a94b9f2aa",
            "94bf0e0be85647cc8ff3efecaab2d651",
            "534c34aaf015439ba8dcf12361a2bd7d",
            "58f8cea9f9dd474aa7b9932a156a9ffc",
            "aead77c155074c2999484ca4cb2e2af6",
            "5a010f34606a49bb84494eeab420b736"
          ]
        },
        "id": "lElFegsSW-IG",
        "outputId": "69fb3c30-56bf-4f04-d033-e3db456f8c0a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "84a3369286a544ff872ab33d8d2a4283"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from drive.MyDrive.llama_analysis.modeling_llama import identify_outliers\n",
        "from drive.MyDrive.llama_analysis.modeling_llama import simple_identify_outliers\n",
        "import tensorflow as tf\n",
        "import torch"
      ],
      "metadata": {
        "id": "KcR9JPKmNqfA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = model.to('cuda')\n",
        "# # shouldn't i be able to put my LLaMA model onto the GPU with 22 GB?"
      ],
      "metadata": {
        "id": "3NH22syEVGwL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model.save_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")\n",
        "#tokenizer.save_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")\n",
        "\n",
        "# for some reason, Llama 3.1 is not working with the custom LlamaForCausalLM."
      ],
      "metadata": {
        "id": "p1NthjH63KKe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## setting up hooks for outlier suppression"
      ],
      "metadata": {
        "id": "A3a01QKdOfeQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# testing out torch.mean cause i'm a brick\n",
        "# so i want everything in hooks to be (1, 4096) basically\n",
        "\n",
        "x = torch.rand(1, 3, 10)\n",
        "print(x)\n",
        "y = torch.rand(1, 3, 10)\n",
        "print(y)\n",
        "\n",
        "print(torch.mean(x, 1))\n",
        "print(\"\\n========\")\n",
        "\n",
        "stack = torch.stack((x,y))\n",
        "\n",
        "print(stack)\n",
        "\n",
        "avged_stack = torch.mean(stack, 2)\n",
        "\n",
        "print(avged_stack)\n",
        "\n",
        "print(torch.mean(avged_stack, 0, True))\n",
        "\n",
        "# print(torch.mean(torch.stack([x,y]), 1))\n",
        "\n",
        "# i have to be averaging across the dimension of token_length(? if you know what i'm trying to get at) because"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nx0j-IxAr4lQ",
        "outputId": "a4efef3f-5ec2-434d-8acf-c02c4245adfa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[4.1271e-01, 2.6455e-01, 9.4175e-01, 4.3693e-01, 5.2515e-02,\n",
            "          6.4193e-01, 2.1367e-01, 6.0808e-01, 7.4872e-01, 2.4860e-01],\n",
            "         [3.0538e-01, 4.5339e-01, 1.2067e-01, 2.8558e-01, 8.8896e-01,\n",
            "          9.1066e-01, 3.8242e-04, 5.9395e-01, 5.2917e-01, 6.2281e-01],\n",
            "         [6.8579e-01, 2.0220e-01, 3.6733e-01, 9.3087e-01, 2.1996e-01,\n",
            "          6.6939e-01, 9.8421e-01, 2.5884e-01, 6.4112e-02, 5.4037e-01]]])\n",
            "tensor([[[0.2824, 0.7217, 0.4506, 0.0372, 0.4099, 0.1212, 0.8269, 0.2789,\n",
            "          0.1141, 0.3892],\n",
            "         [0.2447, 0.4873, 0.7463, 0.9658, 0.8628, 0.1815, 0.4764, 0.5165,\n",
            "          0.7859, 0.3579],\n",
            "         [0.4562, 0.0695, 0.5660, 0.7103, 0.4120, 0.6473, 0.0194, 0.9444,\n",
            "          0.9940, 0.2728]]])\n",
            "tensor([[0.4680, 0.3067, 0.4766, 0.5511, 0.3871, 0.7407, 0.3994, 0.4870, 0.4473,\n",
            "         0.4706]])\n",
            "\n",
            "========\n",
            "tensor([[[[4.1271e-01, 2.6455e-01, 9.4175e-01, 4.3693e-01, 5.2515e-02,\n",
            "           6.4193e-01, 2.1367e-01, 6.0808e-01, 7.4872e-01, 2.4860e-01],\n",
            "          [3.0538e-01, 4.5339e-01, 1.2067e-01, 2.8558e-01, 8.8896e-01,\n",
            "           9.1066e-01, 3.8242e-04, 5.9395e-01, 5.2917e-01, 6.2281e-01],\n",
            "          [6.8579e-01, 2.0220e-01, 3.6733e-01, 9.3087e-01, 2.1996e-01,\n",
            "           6.6939e-01, 9.8421e-01, 2.5884e-01, 6.4112e-02, 5.4037e-01]]],\n",
            "\n",
            "\n",
            "        [[[2.8242e-01, 7.2175e-01, 4.5063e-01, 3.7158e-02, 4.0987e-01,\n",
            "           1.2122e-01, 8.2686e-01, 2.7894e-01, 1.1407e-01, 3.8925e-01],\n",
            "          [2.4469e-01, 4.8725e-01, 7.4628e-01, 9.6577e-01, 8.6280e-01,\n",
            "           1.8150e-01, 4.7641e-01, 5.1651e-01, 7.8588e-01, 3.5789e-01],\n",
            "          [4.5615e-01, 6.9459e-02, 5.6597e-01, 7.1026e-01, 4.1200e-01,\n",
            "           6.4728e-01, 1.9353e-02, 9.4439e-01, 9.9396e-01, 2.7281e-01]]]])\n",
            "tensor([[[0.4680, 0.3067, 0.4766, 0.5511, 0.3871, 0.7407, 0.3994, 0.4870,\n",
            "          0.4473, 0.4706]],\n",
            "\n",
            "        [[0.3278, 0.4262, 0.5876, 0.5711, 0.5616, 0.3167, 0.4409, 0.5799,\n",
            "          0.6313, 0.3400]]])\n",
            "tensor([[[0.3979, 0.3664, 0.5321, 0.5611, 0.4744, 0.5287, 0.4201, 0.5335,\n",
            "          0.5393, 0.4053]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lLH6EvGpx-N7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "from typing import Dict, Callable\n",
        "import torch\n",
        "\n",
        "def remove_all_hooks(model: torch.nn.Module) -> None:\n",
        "    for name, child in model._modules.items():\n",
        "        if child is not None:\n",
        "            if hasattr(child, \"_forward_hooks\"):\n",
        "                child._forward_hooks: Dict[int, Callable] = OrderedDict()\n",
        "            elif hasattr(child, \"_forward_pre_hooks\"):\n",
        "                child._forward_pre_hooks: Dict[int, Callable] = OrderedDict()\n",
        "            elif hasattr(child, \"_backward_hooks\"):\n",
        "                child._backward_hooks: Dict[int, Callable] = OrderedDict()\n",
        "            remove_all_hooks(child)"
      ],
      "metadata": {
        "id": "OGZHPL3FjMct"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remove_all_hooks(model)"
      ],
      "metadata": {
        "id": "hVgtP2b4jVkO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# note: i think i also wrote this wrong again because i should probably have a\n",
        "#   separate averaged stack for the token_len, [1, 4096], so that i can see at\n",
        "#   each layer what is going on. ykwim? not just one 1D tensor the entire time\n",
        "#   lol\n",
        "\n",
        "# !!!!!!\n",
        "# the hook was causing the problem? changing the output somehow?\n",
        "\n",
        "class Hook():\n",
        "  def __init__(self):\n",
        "    self.output = None\n",
        "\n",
        "  def __call__(self, module, input, output):\n",
        "    # torch.mean instead of dumping everything into an output list. that's taking up way too much GPU RAM\n",
        "    processed_output = torch.mean(output.detach(), 1)\n",
        "\n",
        "    if self.output == None:\n",
        "        self.output = processed_output\n",
        "        print(f\"FIRST OUTPUT: {self.output}, {self.output.shape}\")\n",
        "    else:\n",
        "        # this naming is messed up. need to fix, because the avged stack only happens once we get to the self.output.\n",
        "        # i think by doing the torch.mean, i'm essentially squeezing something\n",
        "        stack = torch.cat((self.output, processed_output), dim=0)\n",
        "        # print(f'stack: {stack}')\n",
        "        # avged_stack = torch.mean(stack, 1)\n",
        "        # print(f'avged_stack: {avged_stack}')\n",
        "        self.output = torch.mean(stack, 0, True)\n",
        "        # print(f\"NOW WE OUTPUT: {self.output}\")\n",
        "    # return output\n",
        "\n",
        "  def clear(self):\n",
        "    self.output = None"
      ],
      "metadata": {
        "id": "PiI0dW4XLK_h"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class PreHook():\n",
        "#   def __init__(self):\n",
        "#     self.output = []\n",
        "\n",
        "#   def __call__(self, module, input):\n",
        "#     self.output.append(input)\n",
        "#     return input\n",
        "\n",
        "#   def clear(self):\n",
        "#     self.output = []"
      ],
      "metadata": {
        "id": "6MthoIOb1wp7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hook = Hook()"
      ],
      "metadata": {
        "id": "CBQtdMZS-S2C"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for n, m in model.named_modules():\n",
        "  if \"q_proj\" in n:\n",
        "    # m.register_forward_pre_hook(prehook)\n",
        "    m.register_forward_hook(hook)"
      ],
      "metadata": {
        "id": "T3xsXwEl0d9z"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hook.clear()"
      ],
      "metadata": {
        "id": "FOWsHzHhVEgR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.forward(input_ids=tokenizer(\"Hello, my name is Jenny. I have a husband of four years, and \", return_tensors=\"pt\").input_ids)\n",
        "# wow this makes it a lot easier to see how this whole process of seq -> tokenizer -> ids -> forward should go lol\n",
        "\n",
        "# IGNORE THESE OUTPUTS\n",
        "\n",
        "print(\"done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RBQ_Dl7eA8yr",
        "outputId": "f43f7c36-1739-4490-9ddf-0994757342dc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:drive.MyDrive.llama_analysis.modeling_llama:We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FIRST OUTPUT: tensor([[ 0.1405, -0.6890,  1.3770,  ..., -0.0880,  0.1879,  0.0111]],\n",
            "       device='cuda:0', dtype=torch.float16), torch.Size([1, 4096])\n",
            "done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## mk ii: evaluating perplexity\n",
        "\n",
        "simpler this time, because i gave up on trying to use the lambada dataset and making my own custom functions for all this and just decided to use dataloader"
      ],
      "metadata": {
        "id": "1JJbZy8ZOLYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test[:5%]\")\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\")\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "tokenized_dataset = tokenized_dataset.with_format(\"torch\")\n",
        "\n",
        "# Create DataLoader\n",
        "dataloader = DataLoader(tokenized_dataset, shuffle=False, batch_size=16)"
      ],
      "metadata": {
        "id": "Ph-A9y12EPXY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Inspect a batch\n",
        "# for batch in dataloader:\n",
        "#     print(batch['input_ids'].shape)\n",
        "#     print(batch['attention_mask'].shape)"
      ],
      "metadata": {
        "id": "_Yo7PQltMCUY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation loop\n",
        "model.eval()\n",
        "total_loss = 0\n",
        "total_tokens = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in dataloader:\n",
        "        print(batch[\"input_ids\"])\n",
        "        inputs = batch[\"input_ids\"].clone().detach().to('cuda')\n",
        "        # print(inputs.shape)\n",
        "        # print(inputs) # one concern i have is that a lot of these samples look... exactly the same? which is weird? why\n",
        "        attention_mask = batch[\"attention_mask\"].clone().detach().to('cuda')\n",
        "\n",
        "        outputs = model(inputs, attention_mask=attention_mask, labels=inputs)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        total_loss += loss.item() * inputs.size(1)\n",
        "        total_tokens += inputs.size(0) * inputs.size(1)\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "        print(\"batch done\")\n",
        "\n",
        "# Calculate average loss and perplexity\n",
        "avg_loss = total_loss / total_tokens\n",
        "perplexity = torch.exp(torch.tensor(avg_loss))\n",
        "\n",
        "print(f\"Average Loss: {avg_loss:.4f}\")\n",
        "print(f\"Perplexity: {perplexity:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euVwV11ZKcXe",
        "outputId": "f763b3b6-3343-4809-cd06-7ef3ffe9421b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    1,     2,     2,  ...,     2,     2,     2],\n",
            "        [    1, 29871,   353,  ...,     2,     2,     2],\n",
            "        [    1,     2,     2,  ...,     2,     2,     2],\n",
            "        ...,\n",
            "        [    1,     2,     2,  ...,     2,     2,     2],\n",
            "        [    1, 29871,   353,  ...,     2,     2,     2],\n",
            "        [    1,     2,     2,  ...,     2,     2,     2]])\n",
            "batch done\n",
            "tensor([[    1, 29871,   512,  ...,     2,     2,     2],\n",
            "        [    1, 29871,   350,  ...,     2,     2,     2],\n",
            "        [    1,     2,     2,  ...,     2,     2,     2],\n",
            "        ...,\n",
            "        [    1,     2,     2,  ...,     2,     2,     2],\n",
            "        [    1,     2,     2,  ...,     2,     2,     2],\n",
            "        [    1,     2,     2,  ...,     2,     2,     2]])\n",
            "batch done\n",
            "tensor([[    1, 29871,   353,  ...,     2,     2,     2],\n",
            "        [    1,     2,     2,  ...,     2,     2,     2],\n",
            "        [    1, 29871,  5334,  ...,     2,     2,     2],\n",
            "        ...,\n",
            "        [    1, 29871,   450,  ...,     2,     2,     2],\n",
            "        [    1, 29871,   512,  ...,     2,     2,     2],\n",
            "        [    1, 29871,  3600,  ...,     2,     2,     2]])\n",
            "batch done\n",
            "tensor([[    1, 29871,   512,  ...,     2,     2,     2],\n",
            "        [    1, 29871,   512,  ...,     2,     2,     2],\n",
            "        [    1, 29871,   512,  ...,     2,     2,     2],\n",
            "        ...,\n",
            "        [    1, 29871,   353,  ...,     2,     2,     2],\n",
            "        [    1,     2,     2,  ...,     2,     2,     2],\n",
            "        [    1, 29871,   512,  ...,     2,     2,     2]])\n",
            "batch done\n",
            "tensor([[    1,     2,     2,  ...,     2,     2,     2],\n",
            "        [    1, 29871,   353,  ...,     2,     2,     2],\n",
            "        [    1,     2,     2,  ...,     2,     2,     2],\n",
            "        ...,\n",
            "        [    1,     2,     2,  ...,     2,     2,     2],\n",
            "        [    1, 29871,  4001,  ...,     2,     2,     2],\n",
            "        [    1, 29871,  5334,  ...,     2,     2,     2]])\n",
            "batch done\n",
            "tensor([[    1,     2,     2,  ...,     2,     2,     2],\n",
            "        [    1, 29871,   353,  ...,     2,     2,     2],\n",
            "        [    1,     2,     2,  ...,     2,     2,     2],\n",
            "        ...,\n",
            "        [    1,     2,     2,  ...,     2,     2,     2],\n",
            "        [    1, 29871,   353,  ...,     2,     2,     2],\n",
            "        [    1,     2,     2,  ...,     2,     2,     2]])\n",
            "batch done\n",
            "tensor([[    1, 29871,  7579,  ...,     2,     2,     2],\n",
            "        [    1, 29871,   512,  ...,     2,     2,     2],\n",
            "        [    1, 29871,  2398,  ...,     2,     2,     2],\n",
            "        ...,\n",
            "        [    1, 29871,   353,  ...,     2,     2,     2],\n",
            "        [    1,     2,     2,  ...,     2,     2,     2],\n",
            "        [    1, 29871,   319,  ...,     2,     2,     2]])\n",
            "batch done\n",
            "tensor([[    1, 29871,  5901,  ...,     2,     2,     2],\n",
            "        [    1, 29871,   512,  ...,     2,     2,     2],\n",
            "        [    1,     2,     2,  ...,     2,     2,     2],\n",
            "        ...,\n",
            "        [    1, 29871,   376,  ...,     2,     2,     2],\n",
            "        [    1, 29871,   376,  ...,     2,     2,     2],\n",
            "        [    1,     2,     2,  ...,     2,     2,     2]])\n",
            "batch done\n",
            "tensor([[    1, 29871,   353,  ...,     2,     2,     2],\n",
            "        [    1,     2,     2,  ...,     2,     2,     2],\n",
            "        [    1, 29871,   376,  ...,     2,     2,     2],\n",
            "        ...,\n",
            "        [    1, 29871,  1383,  ...,     2,     2,     2],\n",
            "        [    1, 29871,   450,  ...,     2,     2,     2],\n",
            "        [    1, 29871,  6286,  ...,     2,     2,     2]])\n",
            "batch done\n",
            "tensor([[    1,     2,     2,  ...,     2,     2,     2],\n",
            "        [    1, 29871,   353,  ...,     2,     2,     2],\n",
            "        [    1,     2,     2,  ...,     2,     2,     2],\n",
            "        ...,\n",
            "        [    1, 29871,  8965,  ...,     2,     2,     2],\n",
            "        [    1, 29871, 17773,  ...,     2,     2,     2],\n",
            "        [    1, 29871, 15981,  ...,     2,     2,     2]])\n",
            "batch done\n",
            "tensor([[    1, 29871,  9583,  ...,     2,     2,     2],\n",
            "        [    1, 29871, 17773,  ...,     2,     2,     2],\n",
            "        [    1, 29871,   838,  ...,     2,     2,     2],\n",
            "        ...,\n",
            "        [    1, 29871,   353,  ...,     2,     2,     2],\n",
            "        [    1,     2,     2,  ...,     2,     2,     2],\n",
            "        [    1,     2,     2,  ...,     2,     2,     2]])\n",
            "batch done\n",
            "tensor([[    1,     2,     2,  ...,     2,     2,     2],\n",
            "        [    1, 29871,   353,  ...,     2,     2,     2],\n",
            "        [    1,     2,     2,  ...,     2,     2,     2],\n",
            "        ...,\n",
            "        [    1, 29871,   353,  ...,     2,     2,     2],\n",
            "        [    1,     2,     2,  ...,     2,     2,     2],\n",
            "        [    1, 29871,   450,  ...,     2,     2,     2]])\n",
            "batch done\n",
            "tensor([[    1, 29871,   450,  ...,     2,     2,     2],\n",
            "        [    1, 29871,  7133,  ...,     2,     2,     2],\n",
            "        [    1,     2,     2,  ...,     2,     2,     2],\n",
            "        ...,\n",
            "        [    1, 29871,   512,  ...,     2,     2,     2],\n",
            "        [    1, 29871,  7133,  ...,     2,     2,     2],\n",
            "        [    1,     2,     2,  ...,     2,     2,     2]])\n",
            "batch done\n",
            "tensor([[    1, 29871,   353,  ...,     2,     2,     2],\n",
            "        [    1,     2,     2,  ...,     2,     2,     2],\n",
            "        [    1, 29871,   450,  ...,     2,     2,     2],\n",
            "        ...,\n",
            "        [    1, 29871,  5806,  ...,     2,     2,     2],\n",
            "        [    1,     2,     2,  ...,     2,     2,     2],\n",
            "        [    1, 29871,   353,  ...,     2,     2,     2]])\n",
            "batch done\n",
            "Average Loss: 1.2389\n",
            "Perplexity: 3.4517\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## relic from old era, was trying to figure out the outlier suppression and failing"
      ],
      "metadata": {
        "id": "i8hx1_9LO183"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(hook.output)"
      ],
      "metadata": {
        "id": "c96-03BvqMi5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41df204d-4b34-4966-85e4-8c703e94f718"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# so i can confirm that the MaskedLinear is being loaded\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "id": "n0Zo9ai9ZCpi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5b7943b-5c87-4f64-9cf8-e2cf57f645c0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(32000, 4096)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaSdpaAttention(\n",
            "          (q_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
            "          (k_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
            "          (v_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
            "          (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
            "          (rotary_emb): LlamaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
            "          (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
            "          (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hook.output[0]"
      ],
      "metadata": {
        "id": "88zptEtwxJPq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "088dd55c-102b-4fbc-cf0d-a72a61ceb763"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.1362,  0.0721, -0.0400,  ..., -0.2096, -0.0903,  0.4907],\n",
              "       device='cuda:0', dtype=torch.float16)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for n, m in model.named_modules():\n",
        "#   if \"q_proj\" in n:\n",
        "#     print(m.weight)\n",
        "\n",
        "# weights are irrelevant i'm looking for activations"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7IYTP6s8Zq8B"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(hook.output)"
      ],
      "metadata": {
        "id": "IAJbZLrrHSsO",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb652c17-2b08-41c7-b958-a4f478da93b2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hook.output[0][0] is the first module's activation matrix. for some reason the tensor hook.output[0] is 3D, with the first dimension being size 1, so you just have to unwrap ig\n",
        "\n",
        "test_identify_outliers = hook.output[30]"
      ],
      "metadata": {
        "id": "XnJsdNBk5Fwy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "63e7c9c4-e050-4f66-e054-a45bd7a4149b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "index 30 is out of bounds for dimension 0 with size 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-b84bfd90aafa>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# hook.output[0][0] is the first module's activation matrix. for some reason the tensor hook.output[0] is 3D, with the first dimension being size 1, so you just have to unwrap ig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest_identify_outliers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: index 30 is out of bounds for dimension 0 with size 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_identify_outliers.shape"
      ],
      "metadata": {
        "id": "qyP4VvaXc2Q8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_identify_outliers = test_identify_outliers.squeeze()"
      ],
      "metadata": {
        "id": "AhykTSyRdVD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_identify_outliers"
      ],
      "metadata": {
        "id": "eY9oNBfDdjJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking that identify_outliers works properly\n",
        "\n",
        "# wait but this seems to show that it works?\n",
        "# how come when i bring\n",
        "# print(tf.reduce_sum(tf.cast(identify_outliers(test_identify_outliers, threshold=2.0), tf.float32)))\n",
        "# print(tf.reduce_sum(tf.cast(simple_identify_outliers(test_identify_outliers, threshold=2), tf.float32)))\n",
        "\n",
        "# tens = torch.eye(3) * 3\n",
        "# print(tf.reduce_sum(tf.cast(identify_outliers(tens, threshold=1), tf.float32)))"
      ],
      "metadata": {
        "id": "IDFIZ88XdyWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## some nice little graphing thingies"
      ],
      "metadata": {
        "id": "BemKHChpO9qX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# !!!!\n",
        "# need to fix\n",
        "def create_heatmap(activations, input_text):\n",
        "    # Process activations\n",
        "    activation_matrix = activations.detach().numpy()\n",
        "    print(activation_matrix)\n",
        "    # Create heatmap\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.heatmap(activation_matrix, cmap=\"viridis\", cbar_kws={'label': 'Activation Strength'})\n",
        "    plt.title(f\"LLaMA Layer Activations for: '{input_text}'\")\n",
        "    plt.xlabel(\"Neuron Index\")\n",
        "    plt.ylabel(\"Token\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def create_scatterplot(activations, input_text):\n",
        "    # Process activations\n",
        "    activation_matrix = activations.detach().numpy()\n",
        "    plt"
      ],
      "metadata": {
        "id": "EfMeuzCpMfxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in range(len(hook.output)):\n",
        "#   create_heatmap(hook.output[i][0], \"Hello, my name is Jenny. I have a husband of four years, and \")"
      ],
      "metadata": {
        "id": "plgakjwoNb3d",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# find index of maximum neuron activation for each layer\n",
        "\n",
        "\n",
        "# 30 vs 62\n",
        "module = 24\n",
        "token = 2\n",
        "\n",
        "selected_layer = hook.output[0]\n",
        "\n",
        "print(selected_layer)\n",
        "max_i = torch.argmax(selected_layer)\n",
        "max = torch.max(selected_layer)\n",
        "print(max_i)\n",
        "print(max)"
      ],
      "metadata": {
        "id": "VmEK-RPp6Uki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"Hello, my name is Jenny. I have a husband of four years, and \"\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(selected_layer.detach().cpu().numpy())\n",
        "plt.title(f\"LLaMA Layer Activations for: '{input_text}'\")\n",
        "plt.xlabel(\"Neuron Index\")\n",
        "plt.ylabel(\"Token\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_gXMEP527wcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## trying to evaluate perplexity with lambada. result: perplexity 30000"
      ],
      "metadata": {
        "id": "432JOxoaPB-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "from torch.nn.functional import pad\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "K3l0q9mVvJT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset('lambada', split='validation[:1%]')"
      ],
      "metadata": {
        "id": "BVrKHhI1s9G1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write an evaluator class that will use perplexity\n",
        "\n",
        "class Evaluator:\n",
        "    def __init__(self, dataset, tokenizer):\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        # tokenize the dataset\n",
        "        def tokenize_function(examples):\n",
        "            example = self.tokenizer(examples['text'])\n",
        "            return example\n",
        "\n",
        "        self.dataset = self.dataset.map(tokenize_function, batched=True)\n",
        "        self.dataset.set_format(type='torch', columns=['input_ids'])\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def inference(self, model, batch_size=16):\n",
        "\n",
        "        model.eval()\n",
        "        # The task is to predict the last word of the input.\n",
        "        # total, hit = 0, 0\n",
        "\n",
        "        print(\"Length of dataset:\", len(self.dataset))  # Check dataset length\n",
        "\n",
        "        logits_and_labels = {\n",
        "            'logits': [],\n",
        "            'labels': []\n",
        "        }\n",
        "\n",
        "        for i in range(0, len(self.dataset), batch_size):\n",
        "            batch = self.dataset[i:i+batch_size]\n",
        "            # batch['input_ids'] is a list of tensors\n",
        "\n",
        "            ls_of_input_ids = batch['input_ids']\n",
        "\n",
        "            ls_of_labels = []\n",
        "            truncated_ls_of_input_ids = []\n",
        "\n",
        "            for input_ids in ls_of_input_ids:\n",
        "                ls_of_labels.append(input_ids[-1])\n",
        "                truncated_ls_of_input_ids.append(input_ids[:-1])\n",
        "\n",
        "            tens_2D_padded_input_ids = torch.nn.utils.rnn.pad_sequence(truncated_ls_of_input_ids, batch_first=True, padding_value=2)\n",
        "            labels = torch.stack(ls_of_labels).unsqueeze(1)\n",
        "\n",
        "            print(tens_2D_padded_input_ids.shape)\n",
        "            print(labels.shape)\n",
        "\n",
        "            # torch.stack() works for turning lists into tensors and putting tensors together type shi\n",
        "\n",
        "            ### THIS SHOULD WORK?\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(tens_2D_padded_input_ids) # ok maybe i don't even need to pass in the labels at all\n",
        "                # print(\"Shape of model outputs logits:\", outputs.logits.shape) # Check shape of model outputs\n",
        "                # print(\"Shape of model outputs labels:\", labels.shape) # Check shape of model outputs\n",
        "                logits_and_labels['logits'].append(outputs.logits)\n",
        "                logits_and_labels['labels'].append(labels)\n",
        "                print(len(logits_and_labels['logits']))\n",
        "                print(len(logits_and_labels['labels']))\n",
        "                # outputs.logits.shape is [batch_len, seq_len, vocab_size]. the vocab_size dimension is going to contain the pre-softmax probabilities of the next token.\n",
        "\n",
        "        return logits_and_labels\n",
        "\n",
        "            # i'm going to handle calculating loss, perplexity, the whole shebang in another function\n",
        "\n",
        "\n",
        "\n",
        "##################\n",
        "# all this is shit\n",
        "\n",
        "        # for item in batch:\n",
        "        #     print(type(item))\n",
        "            #print(item['input_ids'].shape)\n",
        "\n",
        "        #     arr_of_input_ids = torch.stack([item['input_ids'] for item in batch]).cuda()\n",
        "        #     input_ids = batch['input_ids'].cuda().unsqueeze(0)\n",
        "        #     # print(\"Shape of input_ids before padding:\", input_ids.shape)  # Add this line\n",
        "        #     label = input_ids[:, -1]\n",
        "        #     pad_len = 512 - input_ids.shape[1]\n",
        "        #     input_ids = pad(input_ids, (0, pad_len), value=1)\n",
        "\n",
        "        #     torch.cuda.empty_cache()\n",
        "\n",
        "        #     outputs = model(input_ids)\n",
        "        #     last_token_logits = outputs.logits[:, -2-(pad_len), :]\n",
        "        #     pred = last_token_logits.argmax(dim=-1)\n",
        "        #     total += label.size(0)\n",
        "        #     hit += (pred == label).sum().item()\n",
        "\n",
        "        # acc = hit / total"
      ],
      "metadata": {
        "id": "pcMnOg72-KE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import gc\n",
        "# gc.collect()"
      ],
      "metadata": {
        "id": "QKBC1EAhyKv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = model.to('cuda')"
      ],
      "metadata": {
        "id": "0xdmtkMkHT6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = Evaluator(dataset, tokenizer)"
      ],
      "metadata": {
        "id": "qnqXWHiut0TW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# no one cares anymore\n",
        "\n",
        "# if tokenizer.pad_token_id is not None:\n",
        "#     pad_token_id = tokenizer.pad_token_id\n",
        "#     print(f\"Padding token ID: {pad_token_id}\")\n",
        "# else:\n",
        "#     print(\"No specific padding token set.\")"
      ],
      "metadata": {
        "id": "o420vK08P2l6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits_and_labels = evaluator.inference(model)\n",
        "\n",
        "# why didn't it go all the way lmfao\n",
        "# i'm dumb i indented the return too far in lmfao\n",
        "# again, another dumb mistake—logits and labels was in the for loop"
      ],
      "metadata": {
        "id": "Jp1US-zVtM6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = logits_and_labels['labels']\n",
        "logits = logits_and_labels['logits']"
      ],
      "metadata": {
        "id": "dPLjxthcX_Je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(logits[0].shape)\n",
        "print(labels[0].shape)"
      ],
      "metadata": {
        "id": "pTXJetDB2h2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "squeezed_labels = [label.squeeze(1) for label in labels]"
      ],
      "metadata": {
        "id": "SER2i2uY4LeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "last_token_logits = [tens[:, -1, :] for tens in logits]"
      ],
      "metadata": {
        "id": "SWX9IDRWZP5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## confused as to why my perplexity is 33000\n",
        "\n",
        "trying to fix that"
      ],
      "metadata": {
        "id": "_37Ex1TSA0Cy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(last_token_logits[0].shape)"
      ],
      "metadata": {
        "id": "Pc8jJnxo3lK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(last_token_logits[0][1])"
      ],
      "metadata": {
        "id": "1PpZBDvN94Lc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(labels[0][1])"
      ],
      "metadata": {
        "id": "YtKQvs72-EkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(last_token_logits[0][1][273])"
      ],
      "metadata": {
        "id": "ezmrDAq4-KKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.plot(last_token_logits[0][1].detach().numpy())\n",
        "plt.title(f\"LLaMA Layer Activations\")\n",
        "plt.xlabel(\"Sample\")\n",
        "plt.ylabel(\"Logit Activation\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "e8yWWcgE9Yb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# i'm not sure why this weird behavior is occurring i'm going to see what loss i get with a different strategy"
      ],
      "metadata": {
        "id": "UNvrwUYw-hll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## loss + perplexity functions"
      ],
      "metadata": {
        "id": "RiWqsKd2A8PE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "calculates loss\n",
        "batch_last_token_logits: [batch_size, vocab_size]\n",
        "squeezed_batch_labels: [batch_size]\n",
        "\n",
        "\"\"\"\n",
        "def calculate_loss(batch_last_token_logits, squeezed_batch_labels):\n",
        "    return F.cross_entropy(input=batch_last_token_logits, target=squeezed_batch_labels)"
      ],
      "metadata": {
        "id": "trmfHVBu2bnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = calculate_loss(last_token_logits[0], squeezed_labels[0])"
      ],
      "metadata": {
        "id": "I03d5Fy54BYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.exp(loss)"
      ],
      "metadata": {
        "id": "o2Qo58Z-4wBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# more manual, crappier version. do not like it. however, it does give the same result.\n",
        "\n",
        "# def calculate_perplexity(batch_logits, batch_labels):\n",
        "#     last_token_logits = batch_logits[:, -1, :]\n",
        "#     probabilities = F.softmax(last_token_logits, dim=-1)\n",
        "#     label_probabilities = torch.gather(input=probabilities, dim=1, index=batch_labels)  # Shape: (batch_size, 1)\n",
        "#     print(label_probabilities)\n",
        "#     nll = -torch.log(label_probabilities)  # Shape: (batch_size, 1)\n",
        "#     mean_nll = nll.mean().item()\n",
        "#     perplexity = torch.exp(torch.tensor(mean_nll)).item()\n",
        "#     return perplexity"
      ],
      "metadata": {
        "id": "GkLu7_dcnmUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(calculate_perplexity(logits[0], labels[0]))\n",
        "\n",
        "# somehow the non-suppressed model is more perplexed lol"
      ],
      "metadata": {
        "id": "mbJVwN_xotZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # DO NOT DO THIS IT BREAKS THE THING\n",
        "\n",
        "# # reset GPU ram\n",
        "\n",
        "# !pip install numba\n",
        "# from numba import cuda\n",
        "# device = cuda.get_current_device()\n",
        "# device.reset()"
      ],
      "metadata": {
        "id": "n2OefRZm-ifV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "IsHv2FAt7PCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UG2hM1n8KfLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.cuda.max_memory_allocated()"
      ],
      "metadata": {
        "id": "gCc97kI8IPeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.cuda.memory_allocated()"
      ],
      "metadata": {
        "id": "iqRPnwWr-5RH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !nvidia-smi"
      ],
      "metadata": {
        "id": "74yanECOGPaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# error : RuntimeError: CUDA error: invalid argument\n",
        "# CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
        "# For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
        "# Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
        "\n",
        "# According to https://discuss.pytorch.org/t/torch-prod-produces-runtimeerror-cuda-driver-error-invalid-argument/179054/17:\n",
        "\n",
        "# The cluster admins have found the solution, which was to clean .cache/torch/kernels which contained a bunch of files like the following:\n",
        "\n",
        "# reduction_prod_arch7.0_nvrtc11.2_sass_29385_edd9f40fe47533f6cc3bc6072826690d02ec24ef\n",
        "\n",
        "# It looks like cleaning it removed the problem altogether."
      ],
      "metadata": {
        "id": "cP4hu5ILGrld"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}